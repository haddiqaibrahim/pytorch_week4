 🚀 PyTorch Week 4 – My Deep Learning Exploration

Hi! I'm Haddiqa Ibrahim, and this repository contains my full learning journey through **Week 4 of the Deep Learning with PyTorch course**, as part of my Data Science Summer 2025 program.

Instead of just watching, I coded along line-by-line, explored the official docs, and built each part in my own words. This repo is **my personal notebook and workspace**, covering everything from tensors to training models on real data.

---

## 🔍 Quick Peek

| 📘 File | 📌 What It Covers |
|--------|-------------------|
| `01_tensors.ipynb` | Creating & manipulating tensors |
| `02_autograd.ipynb` | Computing gradients with autograd |
| `03_dataloader.ipynb` | Building datasets and dataloaders |
| `04_model.ipynb` | Creating a neural network class |
| `05_loss_optimizer.ipynb` | Loss functions & optimizers |
| `06_training_loop.ipynb` | Training a model step-by-step |
| `07_mnist.ipynb` | Real dataset (FashionMNIST) practice |

---

## 💡 Why PyTorch?

I chose PyTorch because it's:
- 🧠 Beginner-friendly and intuitive  
- 🔬 Widely used in research & production  
- 🔄 Offers dynamic computation graphs  
- 🔥 Supports GPU acceleration out of the box  

---

## 📒 What I Practiced

### 🧱 Tensor Playground
- Scalars, vectors, matrices, 3D tensors
- Reshaping, stacking, slicing
- Matrix multiplication rules
- GPU operations and `.to(device)`

### 🔄 Autograd & Backprop
- Setting `requires_grad=True`
- `.backward()` in action
- Freezing layers with `torch.no_grad()`
- Visualizing gradients

### 🧪 Building Neural Nets
- Defined models with `nn.Module`
- Used layers: `Linear`, `ReLU`, `Sigmoid`
- Forward pass logic
- Model summary with `torchinfo`

### 🔄 Training Workflows
- Custom loss and optimizer setup
- Training vs. validation loops
- Epoch-based logging and model evaluation
- Visualized loss using matplotlib

### 🧷 Working with Real Data
- Downloaded FashionMNIST
- Visualized images and predictions
- Measured accuracy and plotted confusion matrix
- Compared different architectures

---

## 📆 Learning Logs (What I Gained)

- I learned how every tensor operation maps to matrix math  
- Understood how `.backward()` actually computes gradients  
- Debugged DataLoader bugs and learned about batching  
- Gained confidence writing full training loops  
- Realized that model performance improves with experimentation  

---

## 🧰 Tech Stack Used

- Python 3.10  
- PyTorch 2.x  
- Torchvision  
- Google Colab  
- Matplotlib  
- Numpy  
- Torchinfo  

---

## 🏁 How to Explore This Repo

1. Clone the repo:
```bash
git clone https://github.com/haddiqaibrahim/pytorch_week4.git
````

2. Open notebooks in [Google Colab](https://colab.research.google.com/) or Jupyter Notebook.

3. Run, experiment, and tweak the models!

---

## 🙋‍♀️ About Me

I'm a curious learner passionate about machine learning, typing fast, and building AI skills one week at a time.
This project is part of my growth, and you're welcome to learn from it too!

🌐 [GitHub](https://github.com/haddiqaibrahim)
🔗 [LinkedIn](https://www.linkedin.com/in/haddiqa-ibrahim/)

---

## 📚 Resources I Used

* [📺 Full PyTorch YouTube Course](https://www.youtube.com/watch?v=Z_ikDlimN6A)
* [📘 PyTorch Official Docs](https://pytorch.org/tutorials/beginner/basics/intro.html)
* \[🧠 Deep Learning Notes – My own handwritten notes (coming soon!)]

---

> 💬 *"Every line of code teaches something new. This is my Week 4 — built with brain, effort, and a little PyTorch magic."* – Haddiqa

