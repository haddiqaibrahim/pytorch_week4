
# ğŸš€ PyTorch Week 4 â€“ My Deep Learning Exploration

Hi! I'm Haddiqa Ibrahim, and this repository contains my full learning journey through Week 4 of the Deep Learning with PyTorch course, as part of my Data Science Summer 2025 program.

Instead of just watching, I coded along line-by-line, explored the official docs, and built each part in my own words. This repo is my personal notebook and workspace, covering everything from tensors to CNNs on real data.

## ğŸ” Quick Peek
| ğŸ“˜ File                      | ğŸ“Œ What It Covers                            |
|----------------------------|----------------------------------------------|
| PyTorch_Tensors.ipynb      | Creating & manipulating tensors              |
| Day2_Tensors.ipynb         | More tensor operations, stacking, reshaping  |
| 03_autograd_model.ipynb    | Gradients with autograd + a basic model      |
| Neural_Network.ipynb       | Defining feedforward neural networks         |
| pytorch_workflow.ipynb     | Full training loop workflow with metrics     |
| CNN_pytorch.ipynb          | Training a CNN on FashionMNIST               |
| Pytorch_fundamentals.ipynb | Core PyTorch concepts + hands-on practice    |

## ğŸ’¡ Why PyTorch?
I chose PyTorch because it's:

- ğŸ§  Beginner-friendly and intuitive  
- ğŸ”¬ Widely used in research & production  
- ğŸ”„ Offers dynamic computation graphs  
- ğŸ”¥ Supports GPU acceleration out of the box  

## ğŸ“’ What I Practiced

### ğŸ§± Tensor Playground
- Scalars, vectors, matrices, 3D tensors  
- Reshaping, stacking, slicing  
- Matrix multiplication rules  
- GPU operations and `.to(device)`

### ğŸ”„ Autograd & Backprop
- Setting `requires_grad=True`  
- `.backward()` in action  
- Freezing layers with `torch.no_grad()`  
- Visualizing gradients  

### ğŸ§ª Building Neural Nets
- Defined models with `nn.Module`  
- Used layers: Linear, ReLU, Sigmoid  
- Forward pass logic  
- Model summary with `torchinfo`  

### ğŸ”„ Training Workflows
- Custom loss and optimizer setup  
- Training vs. validation loops  
- Epoch-based logging and model evaluation  
- Visualized loss using `matplotlib`  

### ğŸ§· Working with Real Data
- Downloaded FashionMNIST  
- Visualized images and predictions  
- Measured accuracy and plotted confusion matrix  
- Built and trained a CNN architecture  

## ğŸ“† Learning Logs (What I Gained)
- I learned how every tensor operation maps to matrix math  
- Understood how `.backward()` actually computes gradients  
- Debugged DataLoader bugs and learned about batching  
- Gained confidence writing full training loops  
- Realized that model performance improves with experimentation  

## ğŸ§° Tech Stack Used
- Python 3.10  
- PyTorch 2.x  
- Torchvision  
- Google Colab  
- Matplotlib  
- Numpy  
- Torchinfo  

## ğŸ How to Explore This Repo

Clone the repo:

```bash
git clone https://github.com/haddiqaibrahim/pytorch_week4.git
````

Open notebooks in Google Colab or Jupyter Notebook.

Run, experiment, and tweak the models!

## ğŸ™‹â€â™€ï¸ About Me

I'm a curious learner passionate about machine learning, typing fast, and building AI skills one week at a time. This project is part of my growth, and you're welcome to learn from it too!

ğŸŒ [GitHub](https://github.com/haddiqaibrahim) ğŸ”— [LinkedIn](https://www.linkedin.com/in/haddiqaibrahim)

## ğŸ“š Resources I Used

* ğŸ“º Full PyTorch YouTube Course
* ğŸ“˜ PyTorch Official Docs

ğŸ’¬ "Every line of code teaches something new. This is my Week 4 â€” built with brain, effort, and a little PyTorch magic." â€“ Haddiqa



