
# 🚀 PyTorch Week 4 – My Deep Learning Exploration

Hi! I'm Haddiqa Ibrahim, and this repository contains my full learning journey through Week 4 of the Deep Learning with PyTorch course, as part of my Data Science Summer 2025 program.

Instead of just watching, I coded along line-by-line, explored the official docs, and built each part in my own words. This repo is my personal notebook and workspace, covering everything from tensors to CNNs on real data.

## 🔍 Quick Peek
| 📘 File                      | 📌 What It Covers                            |
|----------------------------|----------------------------------------------|
| PyTorch_Tensors.ipynb      | Creating & manipulating tensors              |
| Day2_Tensors.ipynb         | More tensor operations, stacking, reshaping  |
| 03_autograd_model.ipynb    | Gradients with autograd + a basic model      |
| Neural_Network.ipynb       | Defining feedforward neural networks         |
| pytorch_workflow.ipynb     | Full training loop workflow with metrics     |
| CNN_pytorch.ipynb          | Training a CNN on FashionMNIST               |
| Pytorch_fundamentals.ipynb | Core PyTorch concepts + hands-on practice    |

## 💡 Why PyTorch?
I chose PyTorch because it's:

- 🧠 Beginner-friendly and intuitive  
- 🔬 Widely used in research & production  
- 🔄 Offers dynamic computation graphs  
- 🔥 Supports GPU acceleration out of the box  

## 📒 What I Practiced

### 🧱 Tensor Playground
- Scalars, vectors, matrices, 3D tensors  
- Reshaping, stacking, slicing  
- Matrix multiplication rules  
- GPU operations and `.to(device)`

### 🔄 Autograd & Backprop
- Setting `requires_grad=True`  
- `.backward()` in action  
- Freezing layers with `torch.no_grad()`  
- Visualizing gradients  

### 🧪 Building Neural Nets
- Defined models with `nn.Module`  
- Used layers: Linear, ReLU, Sigmoid  
- Forward pass logic  
- Model summary with `torchinfo`  

### 🔄 Training Workflows
- Custom loss and optimizer setup  
- Training vs. validation loops  
- Epoch-based logging and model evaluation  
- Visualized loss using `matplotlib`  

### 🧷 Working with Real Data
- Downloaded FashionMNIST  
- Visualized images and predictions  
- Measured accuracy and plotted confusion matrix  
- Built and trained a CNN architecture  

## 📆 Learning Logs (What I Gained)
- I learned how every tensor operation maps to matrix math  
- Understood how `.backward()` actually computes gradients  
- Debugged DataLoader bugs and learned about batching  
- Gained confidence writing full training loops  
- Realized that model performance improves with experimentation  

## 🧰 Tech Stack Used
- Python 3.10  
- PyTorch 2.x  
- Torchvision  
- Google Colab  
- Matplotlib  
- Numpy  
- Torchinfo  

## 🏁 How to Explore This Repo

Clone the repo:

```bash
git clone https://github.com/haddiqaibrahim/pytorch_week4.git
````

Open notebooks in Google Colab or Jupyter Notebook.

Run, experiment, and tweak the models!

## 🙋‍♀️ About Me

I'm a curious learner passionate about machine learning, typing fast, and building AI skills one week at a time. This project is part of my growth, and you're welcome to learn from it too!

🌐 [GitHub](https://github.com/haddiqaibrahim) 🔗 [LinkedIn](https://www.linkedin.com/in/haddiqaibrahim)

## 📚 Resources I Used

* 📺 Full PyTorch YouTube Course
* 📘 PyTorch Official Docs

💬 "Every line of code teaches something new. This is my Week 4 — built with brain, effort, and a little PyTorch magic." – Haddiqa



