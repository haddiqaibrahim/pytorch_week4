 ðŸš€ PyTorch Week 4 â€“ My Deep Learning Exploration

Hi! I'm Haddiqa Ibrahim, and this repository contains my full learning journey through **Week 4 of the Deep Learning with PyTorch course**, as part of my Data Science Summer 2025 program.

Instead of just watching, I coded along line-by-line, explored the official docs, and built each part in my own words. This repo is **my personal notebook and workspace**, covering everything from tensors to training models on real data.

---

## ðŸ” Quick Peek

| ðŸ“˜ File | ðŸ“Œ What It Covers |
|--------|-------------------|
| `01_tensors.ipynb` | Creating & manipulating tensors |
| `02_autograd.ipynb` | Computing gradients with autograd |
| `03_dataloader.ipynb` | Building datasets and dataloaders |
| `04_model.ipynb` | Creating a neural network class |
| `05_loss_optimizer.ipynb` | Loss functions & optimizers |
| `06_training_loop.ipynb` | Training a model step-by-step |
| `07_mnist.ipynb` | Real dataset (FashionMNIST) practice |

---

## ðŸ’¡ Why PyTorch?

I chose PyTorch because it's:
- ðŸ§  Beginner-friendly and intuitive  
- ðŸ”¬ Widely used in research & production  
- ðŸ”„ Offers dynamic computation graphs  
- ðŸ”¥ Supports GPU acceleration out of the box  

---

## ðŸ“’ What I Practiced

### ðŸ§± Tensor Playground
- Scalars, vectors, matrices, 3D tensors
- Reshaping, stacking, slicing
- Matrix multiplication rules
- GPU operations and `.to(device)`

### ðŸ”„ Autograd & Backprop
- Setting `requires_grad=True`
- `.backward()` in action
- Freezing layers with `torch.no_grad()`
- Visualizing gradients

### ðŸ§ª Building Neural Nets
- Defined models with `nn.Module`
- Used layers: `Linear`, `ReLU`, `Sigmoid`
- Forward pass logic
- Model summary with `torchinfo`

### ðŸ”„ Training Workflows
- Custom loss and optimizer setup
- Training vs. validation loops
- Epoch-based logging and model evaluation
- Visualized loss using matplotlib

### ðŸ§· Working with Real Data
- Downloaded FashionMNIST
- Visualized images and predictions
- Measured accuracy and plotted confusion matrix
- Compared different architectures

---

## ðŸ“† Learning Logs (What I Gained)

- I learned how every tensor operation maps to matrix math  
- Understood how `.backward()` actually computes gradients  
- Debugged DataLoader bugs and learned about batching  
- Gained confidence writing full training loops  
- Realized that model performance improves with experimentation  

---

## ðŸ§° Tech Stack Used

- Python 3.10  
- PyTorch 2.x  
- Torchvision  
- Google Colab  
- Matplotlib  
- Numpy  
- Torchinfo  

---

## ðŸ How to Explore This Repo

1. Clone the repo:
```bash
git clone https://github.com/haddiqaibrahim/pytorch_week4.git
````

2. Open notebooks in [Google Colab](https://colab.research.google.com/) or Jupyter Notebook.

3. Run, experiment, and tweak the models!

---

## ðŸ™‹â€â™€ï¸ About Me

I'm a curious learner passionate about machine learning, typing fast, and building AI skills one week at a time.
This project is part of my growth, and you're welcome to learn from it too!

ðŸŒ [GitHub](https://github.com/haddiqaibrahim)
ðŸ”— [LinkedIn](https://www.linkedin.com/in/haddiqa-ibrahim/)

---

## ðŸ“š Resources I Used

* [ðŸ“º Full PyTorch YouTube Course](https://www.youtube.com/watch?v=Z_ikDlimN6A)
* [ðŸ“˜ PyTorch Official Docs](https://pytorch.org/tutorials/beginner/basics/intro.html)
* \[ðŸ§  Deep Learning Notes â€“ My own handwritten notes (coming soon!)]

---

> ðŸ’¬ *"Every line of code teaches something new. This is my Week 4 â€” built with brain, effort, and a little PyTorch magic."* â€“ Haddiqa

